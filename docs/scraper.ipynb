{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Subreddits\n",
    "\n",
    "Demonstration of scraping subreddits and saving them to CSV or database.\n",
    "\n",
    "### Running the `scraper` Jupyter Notebook\n",
    "\n",
    "1. Choose subreddits to scrape in `subreddit_list`\n",
    "2. Run the scraper with save option as `csv` or `sqlite`.\n",
    "3. Optional to use the returned df from scrape without saving.\n",
    "\n",
    "### Running the `scraper.py` file\n",
    "\n",
    "```bash\n",
    "# Scrape 'datascience' and 'sql':\n",
    "# Defaults: save='csv', sort='new'\n",
    "$ python scraper.py datascience sql\n",
    "\n",
    "# Scrape with save and sort options:\n",
    "$ python scraper.py datascience sql --save sqlite --sorting rising\n",
    "\n",
    "# Scrape using configuration file:\n",
    "$ python scraper.py --config scraper_config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import datetime\n",
    "import os\n",
    "import argparse\n",
    "import databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "           'accept-encoding': 'gzip, deflate, sdch, br',\n",
    "           'accept-language': 'en-GB,en;q=0.8,en-US;q=0.6,ml;q=0.4',\n",
    "           'cache-control': 'max-age=0',\n",
    "           'upgrade-insecure-requests': '1',\n",
    "           'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.date = str(datetime.datetime.now().date())\n",
    "\n",
    "    def scrape_subreddit(self, subreddit_list, sorting='new'):\n",
    "        '''Scrapes a subreddit for post titles.\n",
    "\n",
    "            subname: subreddit name to scrape\n",
    "\n",
    "            pages: pages to scrape, typically 25 posts per page. Default is 42 for partial page scrape buffer.\n",
    "            Reddit seems to have a soft limit of 1000 posts, can't seem to get around it.\n",
    "\n",
    "            sorting: possible sort orders\n",
    "                - new\n",
    "                - rising\n",
    "                - controversial\n",
    "                - top\n",
    "        '''\n",
    "        self.sorting = sorting\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for sub in subreddit_list:\n",
    "\n",
    "            url = f'https://old.reddit.com/r/{sub}/{sorting}.json'\n",
    "            post_titles = []\n",
    "            prev_post_len = 0\n",
    "            curr_post_len = 0\n",
    "            after = None\n",
    "            print(f'Scraping subreddit \"{sub}\"')\n",
    "\n",
    "            while (prev_post_len == 0) or (prev_post_len != curr_post_len):\n",
    "                prev_post_len = curr_post_len\n",
    "                if after == None:\n",
    "                    params = {}\n",
    "                else:\n",
    "                    params = {'after': after}\n",
    "                response = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "                if response.status_code != 200:\n",
    "                    print('Error:', response.status_code)\n",
    "                    break\n",
    "\n",
    "                post_json = response.json()\n",
    "                for post in post_json['data']['children']:\n",
    "                    title = post['data']['title']\n",
    "                    if title not in post_titles:\n",
    "                        post_titles.append(title)\n",
    "                curr_post_len = len(post_titles)\n",
    "\n",
    "                after = post_json['data']['after']\n",
    "                sleep(.5)\n",
    "\n",
    "            print(f'Success. {len(post_titles)} total posts for \"{sub}\"')\n",
    "\n",
    "            data = pd.DataFrame(\n",
    "                data={'title': post_titles, 'subreddit': sub, 'date': self.date})\n",
    "            df = pd.concat([df, data], ignore_index=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def save_to_csv(self, df):\n",
    "\n",
    "        date = str(datetime.datetime.now().date())\n",
    "\n",
    "        if not os.path.exists('../scraped_subreddits/'):\n",
    "            os.mkdir('../scraped_subreddits/')\n",
    "\n",
    "        for sub in df.subreddit.unique():\n",
    "            mask = df['subreddit'] == sub\n",
    "            sub_df = df[mask]\n",
    "            df.to_csv(\n",
    "                f'../scraped_subreddits/{sub}_{self.sorting}_{self.date}.csv', index=False)\n",
    "            print(f'Saved \"{sub}\" to CSV')\n",
    "\n",
    "    def save_to_sqlite(self, df):\n",
    "        db = databases.Sqlite()\n",
    "        connection = db.create_connection('reddit.sqlite')\n",
    "\n",
    "        create_subreddits_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS subreddits (\n",
    "          id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "          title TEXT NOT NULL,\n",
    "          subreddit TEXT NOT NULL,\n",
    "          date TEXT NOT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "\n",
    "        db.execute_query(connection, create_subreddits_table)\n",
    "        df.to_sql(name='subreddits', con=connection,\n",
    "                  if_exists='append', index=False)\n",
    "        print('Data saved to sqlite database successfully')\n",
    "\n",
    "    def save_to_postgres(self, df):\n",
    "        return 'save to postgres'\n",
    "        # YEAH\n",
    "\n",
    "    def save_to_mongo(self, df):\n",
    "        return 'save to mongo'\n",
    "        # definitely a FUTURE addition\n",
    "\n",
    "    def save_to_mysql(self, df):\n",
    "        return 'save to mysql'\n",
    "        # for django or something\n",
    "\n",
    "    def save_choice(self, choice):\n",
    "        '''\n",
    "        Choice to save the scraped dataframe.\n",
    "        Choices:\n",
    "        'csv', 'sqlite', 'postgres', 'mongo', 'mysql'\n",
    "        '''\n",
    "        save_options = {\n",
    "            'csv': self.save_to_csv,\n",
    "            'sqlite': self.save_to_sqlite,\n",
    "            'postgres': self.save_to_postgres,\n",
    "            'mongo': self.save_to_mongo,\n",
    "            'mysql': self.save_to_mysql\n",
    "        }\n",
    "        save_function = save_options.get(choice, self.save_to_csv)\n",
    "        return save_function(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subreddits to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_list = ['datascience', 'dataengineering']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape and Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape = Scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping subreddit \"datascience\"\n",
      "Success. 904 total posts for \"datascience\"\n",
      "Scraping subreddit \"dataengineering\"\n",
      "Success. 977 total posts for \"dataengineering\"\n"
     ]
    }
   ],
   "source": [
    "df = scrape.scrape_subreddit(subreddit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved \"datascience\" to CSV\n",
      "Saved \"dataengineering\" to CSV\n"
     ]
    }
   ],
   "source": [
    "scrape.save_to_csv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to SQLite DB successful\n",
      "Query executed successfully\n",
      "Data saved to sqlite database successfully\n"
     ]
    }
   ],
   "source": [
    "scrape.save_to_sqlite(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For running as .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser(prog='Subreddit Scraper',\n",
    "                                     description='''\n",
    "                                     Choose subreddits to scrape, how to sort them, and how to save them.\n",
    "                                     Sort order options: \"new\", \"rising\", \"controversial\", \"top\". Default is \"new\".\n",
    "                                     Save option is \"csv\", \"sqlite\", \"postgres\", \"mongo\", \"mysql\". Default is \"csv\".''')\n",
    "\n",
    "    parser.add_argument('--config', action='store',\n",
    "                        help='Configuration file for scraper')\n",
    "    parser.add_argument('-subs', '--subreddit_list', action='store', nargs='+',\n",
    "                        help='Subreddits to scrape, no quotes, no brackets')\n",
    "    parser.add_argument('--sorting', action='store', choices=['new', 'rising', 'controversial', 'top'],\n",
    "                        default='new', help='Sort order for subreddits to scrape.')\n",
    "    parser.add_argument('--save', action='store', choices=['csv', 'sqlite', 'postgres', 'mongo', 'mysql'],\n",
    "                        default='csv', help='How/where to save scraped subreddit.')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.config:\n",
    "        config_file = json.load(open(args.config))\n",
    "        subreddit_list = config_file['subreddit_list']\n",
    "        sorting = config_file['sorting']\n",
    "        save_location = config_file['save_location']\n",
    "    else:\n",
    "        subreddit_list = args.subreddit_list\n",
    "        sorting = args.sorting\n",
    "        save_location = args.save\n",
    "\n",
    "    scrape = Scraper()\n",
    "    df = scrape.scrape_subreddit(subreddit_list, sorting=sorting)\n",
    "    scrape.save_choice(save_location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
