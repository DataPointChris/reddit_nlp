{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from PIL import Image\n",
    "import wordcloud\n",
    "import time\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from glob import glob\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import databases\n",
    "from helpers import dataloader\n",
    "from helpers import grid_models\n",
    "from helpers.reddit_functions import Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_list = ['datascience','machinelearning','dataengineering','python','aws']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataloader.data_selector(subreddit_list, 'sqlite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of list items with no data retrieved\n",
    "subreddit_list = [sub for sub in subreddit_list if sub in df.subreddit.unique()]\n",
    "subreddit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataloader.subreddit_encoder(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "y = df['sub_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('count_vec', CountVectorizer()),(('lr', LogisticRegression()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "                'count_vec__max_features': [4000],\n",
    "                'count_vec__max_df': [.3],\n",
    "                'count_vec__ngram_range': [(1,2)],\n",
    "                'count_vec__stop_words': [custom_stop_words],\n",
    "                'count_vec__min_df': [3],\n",
    "                'lr__penalty': ['l2'],\n",
    "                'lr__C': [5]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GridSearchCV(pipe, param_grid=pipe_params, cv=5, verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIX ###\n",
    "# Hardcoded count_vec\n",
    "# Test for tfidf, may have to hardcode into ds_workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model is GridSearchCV\n",
    "# best_estimator is Pipeline\n",
    "# named_steps is the steps in the pipeline\n",
    "# count_vec is the FITTED \n",
    "\n",
    "features_data = model.best_estimator_.named_steps.count_vec.transform(X).toarray()\n",
    "features_columns = model.best_estimator_.named_steps.count_vec.get_feature_names()\n",
    "features_df = pd.DataFrame(data=features_data, columns=features_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK ### does this function work with tfidf, or just count vec?\n",
    "def plot_most_common(df, features_df, subreddit_list=subreddit_list, num_features=20, standardize=False, include_combined=False):\n",
    "    '''\n",
    "    Plots the most common features for each subreddit in the DataFrame\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    df: original DataFrame\n",
    "\n",
    "    features_df: should be output from transformer on df\n",
    "\n",
    "        Example:\n",
    "        features_df = pd.DataFrame(\n",
    "                                data={transformer}.transform(X).toarray(),\n",
    "                                columns={transformer}.get_feature_names())\n",
    "\n",
    "    num_features: number of most common features to plot for each subreddit\n",
    "\n",
    "    standardize: put all of the plots on the same scale\n",
    "\n",
    "    combined: include a plot of the most common features of all of the subreddits combined\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    plots\n",
    "\n",
    "    '''\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=1,\n",
    "                           nrows=len(subreddit_list) + int(1 if include_combined else 0),\n",
    "                           figsize=(15, num_features/1.3*len(subreddit_list)))\n",
    "\n",
    "    for subplot_idx, sub in enumerate(subreddit_list):\n",
    "        sub_features = features_df.loc[df['subreddit'] == sub]\n",
    "        sub_top_words = sub_features.sum().sort_values(ascending=False).head(num_features)[::-1]\n",
    "        sub_top_words.plot(kind='barh', ax=ax[subplot_idx])\n",
    "        ax[subplot_idx].set_title(f'{num_features} Most Common Words for {sub.upper()}', fontsize=16)\n",
    "        \n",
    "        if standardize:\n",
    "            max_occurence = features_df.sum().max()*1.02\n",
    "            ax[subplot_idx].set_xlim(0, max_occurence)\n",
    "\n",
    "    if include_combined:\n",
    "        most_common = features_df.sum().sort_values(ascending=False).head(num_features)[::-1]\n",
    "        most_common.plot(kind='barh', ax=ax[subplot_idx+1])\n",
    "        ax[subplot_idx+1].set_title(f'{num_features} Most Common Words for ({\", \".join(subreddit_list).upper()})')\n",
    "        \n",
    "        if standardize:\n",
    "            ax[subplot_idx+1].set_xlim(0, max_occurence)\n",
    "    \n",
    "    plt.tight_layout(h_pad=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_common(df, features_df, num_features=15, include_combined=True, standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###HELP### I don't think this is working right\n",
    "###HELP### coefficients don't make sense for the entire dataset, would need to do one for each thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###NOTE### coefficients only for lr and etc etc etc...\n",
    "###NOTE### coefficients only for two subreddits\n",
    "\n",
    "### FIX ### look at the coef_ portion of the new single model instead of the gridsearch\n",
    "\n",
    "\n",
    "# [-1][1] for last step (estimator)(instantiation)\n",
    "# coef_[0]because I don't know why\n",
    "coefs = model.best_estimator_.steps[-1][1].coef_[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.title('Feature Importance (Coefficients)', fontsize=20)\n",
    "plt.ylabel('Features', fontsize=18)\n",
    "plt.xlabel('(Abs) Coefficients', fontsize=18)\n",
    "\n",
    "coef_df = pd.DataFrame(data=[coefs], columns=features_columns).T\n",
    "coef_df['abs_coef'] = coef_df[0].abs()\n",
    "coef_df.sort_values('abs_coef', ascending=False)[0].head(15).plot(kind='barh');\n",
    "\n",
    "# coef_kill = coef_df.sort_values('abs_coef', ascending=False)[0].head(500).index\n",
    "\n",
    "# coef_kill\n",
    "\n",
    "\n",
    "# my_stops.extend(coef_kill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(subreddit_list):\n",
    "    '''\n",
    "    Makes combination pairs of subreddits from subreddit_list\n",
    "    '''\n",
    "    if len(subreddit_list) > 2:\n",
    "            return list(combinations(subreddit_list, 2))\n",
    "    return subreddit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = make_pairs(subreddit_list)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_common_pairs(df, features_df, pairs, num_features=20):\n",
    "    '''\n",
    "    Plots the most common features for each subreddit in the DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    df: original DataFrame\n",
    "    \n",
    "    features_df: should be output from transformer on df\n",
    "        \n",
    "        Example:\n",
    "        features_df = pd.DataFrame(\n",
    "                                data={transformer}.transform(X).toarray(),\n",
    "                                columns={transformer}.get_feature_names())\n",
    "    \n",
    "    num_features: number of most common features to plot for each subreddit\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    plots\n",
    "    \n",
    "    '''\n",
    "    fig, ax = plt.subplots(ncols=2, \n",
    "                           nrows=len(pairs), \n",
    "                           figsize=(16,num_features/3*len(pairs)))\n",
    "\n",
    "    for i, pair in enumerate(pairs):\n",
    "\n",
    "        # features for each pair\n",
    "        feats_0 = features_df.loc[(df['subreddit'] == pair[0])]\n",
    "        feats_1 = features_df.loc[(df['subreddit'] == pair[1])]\n",
    "        # combined\n",
    "        common_feats = feats_0.append(feats_1)\n",
    "        # this is the most common between the two\n",
    "        most_common = common_feats.sum().sort_values(ascending=False).head(num_features)[::-1]\n",
    "        # plot\n",
    "        feats_0[most_common.index].sum().plot.barh(ax=ax[i, 0], color='navy')\n",
    "        feats_1[most_common.index].sum().plot.barh(ax=ax[i, 1], color='orange')\n",
    "        ax[i, 0].set_title(f'Top {num_features} - {pair} \\nSub: {pair[0].upper()}', fontsize=16, wrap=True)\n",
    "        ax[i, 1].set_title(f'Top {num_features} - {pair} \\nSub: {pair[1].upper()}', fontsize=16, wrap=True)\n",
    "        max_occurence = common_feats.sum().max()*1.02\n",
    "        ax[i, 0].set_xlim(0,max_occurence)\n",
    "        ax[i, 1].set_xlim(0,max_occurence)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_common_pairs(df, features_df, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = features_df.sum().sort_values(ascending=False).head(20)[::-1]\n",
    "groups = features_df.groupby(df['subreddit']).sum()[most_common.index].T.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, figsize=(18,20))\n",
    "\n",
    "groups.plot.bar(ax=ax[0], width=.8, fontsize=15)\n",
    "ax[0].set_title('20 Most Common Words', fontsize=20)\n",
    "ax[0].set_ylabel('# of Occurences', fontsize=15)\n",
    "ax[0].legend(fontsize=15, fancybox=True, framealpha=1, shadow=True, borderpad=1)\n",
    "\n",
    "groups.plot(kind='bar', ax=ax[1], width=.35, fontsize=15, stacked=True)\n",
    "ax[1].set_title('20 Most Common Words', fontsize=20)\n",
    "ax[1].set_ylabel('# of Occurences', fontsize=15)\n",
    "ax[1].legend(fontsize=15, fancybox=True, framealpha=1, shadow=True, borderpad=1)\n",
    "\n",
    "\n",
    "plt.tight_layout(h_pad=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE ### this does not use the X value inputted when using split...\n",
    "\n",
    "\n",
    "def make_cloud(X, height=300, width=800, max_words=100, split=None, labels=None, stopwords=None, colormap='viridis', background_color='black'):\n",
    "    '''\n",
    "    Inputs:\n",
    "    X: text input\n",
    "    height: height of each wordcloud\n",
    "    width: width of each wordcloud\n",
    "    max_words: max words for each wordcloud\n",
    "    split: if True, wordcloud for each subreddit\n",
    "    labels: must provide list of labels if split=True, to generate a wordcloud for each label\n",
    "    stopwords: usually these are the same stopwords used by the tranformer (CountVectorizer or Tfidf)\n",
    "    colormap: any choice from matplotlib gallery.  Find them with plt.cm.datad\n",
    "        'random': picks a random colormap for each cloud.\n",
    "    '''\n",
    "\n",
    "    colormaps = [m for m in plt.cm.datad if not m.endswith(\"_r\")]\n",
    "    wc = wordcloud.WordCloud(max_words=max_words,\n",
    "                             width=width,\n",
    "                             height=height,\n",
    "                             background_color=background_color,\n",
    "                             colormap=np.random.choice(\n",
    "                                 colormaps) if colormap == 'random' else colormap,\n",
    "                             stopwords=stopwords)\n",
    "    if split:\n",
    "        for label in labels:\n",
    "            cloud = wc.generate(\n",
    "                df[df['subreddit'] == label]['title'].str.cat())\n",
    "            plt.figure(figsize=(width/100, height*len(labels)/100), dpi=100)\n",
    "            plt.title(label.upper(), fontdict={'fontsize': 15})\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(cloud.to_image(), interpolation='bilinear')\n",
    "\n",
    "    else:\n",
    "        cloud = wc.generate(X.str.cat())\n",
    "        return cloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_cloud(X, stopwords=custom_stop_words, colormap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_cloud(X, split=True, labels=subreddit_list, stopwords=custom_stop_words, colormap='random', background_color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('../images/reddit03.png')\n",
    "gray = np.array(img.convert('L'))\n",
    "mask = np.where(gray < 200, 255, 0)\n",
    "\n",
    "wc = wordcloud.WordCloud(background_color='white', \n",
    "                         max_words=500, \n",
    "                         mask=mask, \n",
    "                         colormap='Reds',\n",
    "                         contour_color='orangered',\n",
    "                         contour_width=1,\n",
    "                         stopwords=custom_stop_words)\n",
    "wc.generate(X.str.cat())\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = Image.open('../images/reddit02.jpg')\n",
    "gray2 = np.array(img2.convert('L'))\n",
    "mask2 = np.where(gray2 < 200, 255, 0)\n",
    "\n",
    "wc2 = wordcloud.WordCloud(background_color='white', \n",
    "                         max_words=1000, \n",
    "                         mask=mask2, \n",
    "                         colormap='Reds',\n",
    "                         contour_color='orangered',\n",
    "                         contour_width=1,\n",
    "                         stopwords=custom_stop_words)\n",
    "wc2.generate(X.str.cat())\n",
    "plt.figure(figsize=(8,12))\n",
    "plt.imshow(wc2, interpolation='bilinear')\n",
    "plt.axis(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array(Image.open(\"../images/reddit06.jpg\"))\n",
    "colorcloud = wordcloud.WordCloud(stopwords=custom_stop_words,\n",
    "                                 background_color=\"white\",\n",
    "                                 mode=\"RGBA\",\n",
    "                                 max_words=1000,\n",
    "                                 mask=mask)\n",
    "colorcloud.generate(X.str.cat())\n",
    "\n",
    "image_colors = wordcloud.ImageColorGenerator(mask)\n",
    "plt.figure(figsize=[7, 7])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
    "\n",
    "ax1.imshow(mask)\n",
    "ax1.axis(False)\n",
    "\n",
    "ax2.imshow(colorcloud.recolor(color_func=image_colors),\n",
    "           interpolation=\"bilinear\")\n",
    "ax2.axis(False)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(cm, columns=subreddit_list, index=subreddit_list)\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "df_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "sns.set(font_scale=2)\n",
    "sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, cmap='Greens', ax=ax, )\n",
    "fontdict={'fontsize': 16}\n",
    "ax.set_yticklabels(labels=subreddit_list, rotation='horizontal', fontdict=fontdict)\n",
    "ax.set_xticklabels(labels=subreddit_list, rotation=20, fontdict=fontdict)\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position('top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "mtn = mcm[:, 0, 0]\n",
    "mtp = mcm[:, 1, 1]\n",
    "mfn = mcm[:, 1, 0]\n",
    "mfp = mcm[:, 0, 1]\n",
    "print(mcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=len(subreddit_list),\n",
    "                       figsize=(12, 6*len(subreddit_list)))\n",
    "\n",
    "for i, cm in enumerate(mcm):\n",
    "    df_cm = pd.DataFrame(cm)\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted'\n",
    "    sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False,\n",
    "                cmap='Purples', ax=ax[i, 0])\n",
    "\n",
    "    ax[i, 0].set_yticklabels(labels=[1, 0], rotation='horizontal')\n",
    "    ax[i, 0].set_xticklabels(labels=[1, 0])\n",
    "    ax[i, 0].xaxis.tick_top()\n",
    "    ax[i, 0].xaxis.set_label_position('top')\n",
    "    ax[i, 0].set_title(subreddit_list[i].upper())\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specif = tn / (tn + fp)\n",
    "\n",
    "    sens = tp / (tp + fn)\n",
    "    box_text = f'''Subreddit: {subreddit_list[i].upper()}\\n\\nSpecificity: {round(specif,4)}\\n\\nSensitivity: {round(sens,4)}'''\n",
    "    ax[i, 1].text(0.5, 0.5, box_text, horizontalalignment='center',\n",
    "                  verticalalignment='center', fontsize=24)\n",
    "    ax[i, 1].set_axis_off()\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, digits=3, target_names=subreddit_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TODO:</h1>\n",
    "\n",
    "1. Make it have the roc for each sub, have to get into the original df where subname equals indexes?\n",
    "2. Plot confusion matrix\n",
    "3. Make a notebook to test the confusion matrixes one by one with each individual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y_test, y_pred)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not multiclass\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (area = {auc})', color='r', marker='D')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', size=16)\n",
    "plt.ylabel('True Positive Rate (Sensitivity)', size=16)\n",
    "plt.title('ROC Curve', size=20)\n",
    "plt.legend(fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.normal(0, 0.8, 1000) x2 = np.random.normal(-2, 1, 1000) x3 = np.random.normal(3, 2, 1000)\n",
    "kwargs = dict(histtype='stepfilled', alpha=0.3, normed=True, bins=40)\n",
    "plt.hist(x1, **kwargs) plt.hist(x2, **kwargs) plt.hist(x3, **kwargs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create histogram of observations.\n",
    "plt.hist(probs, bins=25, color='b')\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Distribution of P(Outcome = 1)', fontsize=22)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xlabel('Predicted Probability that Outcome = 1', fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create two histograms of observations.\n",
    "hst0 = plt.hist(probs[y_test == 0],\n",
    "         bins=25,\n",
    "         color='b',\n",
    "         alpha = 0.6,\n",
    "         label='Technology',)\n",
    "\n",
    "hst1 = plt.hist(probs[y_test == 1],\n",
    "         bins=25,\n",
    "         color='orange',\n",
    "         alpha = 0.6,\n",
    "         label='Science')\n",
    "\n",
    "# Add vertical line at P(Outcome = 1) = 0.5.\n",
    "plt.vlines(x=0.5,\n",
    "           ymin = 0,\n",
    "           ymax = max(hst1[0].max(), hst0[0].max()), # Max of the two highest respective hist values\n",
    "           color='r',\n",
    "           linestyle = '--')\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Distribution of P(Science)', fontsize=22)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xlabel('Predicted Probability', fontsize=18)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=20);\n",
    "\n",
    "# Thanks to Matt Brems for the colorful graphs! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
