{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Workflow\n",
    "## Find the Best Model\n",
    "\n",
    "This notebook shows how to use some of the functions located in `reddit_functions` to compare the performance of different models on the data.\n",
    "\n",
    "A second workflow is included to take the parameters of the best model and create a new model and fit it on the entire dataset and see the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helpers import databases\n",
    "from helpers import dataloader\n",
    "from helpers import grid_models\n",
    "from helpers.reddit_functions import Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_list = ['css', 'html', 'javascript', 'datascience', 'machinelearning', 'etl', 'python', 'dataengineering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subreddit_list = ['datascience','machinelearning','dataengineering','python','aws','sql']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to SQLite DB successful\n"
     ]
    }
   ],
   "source": [
    "df = dataloader.data_selector(subreddit_list, 'sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['css',\n",
       " 'html',\n",
       " 'javascript',\n",
       " 'datascience',\n",
       " 'machinelearning',\n",
       " 'etl',\n",
       " 'python',\n",
       " 'dataengineering']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get rid of list items with no data retrieved\n",
    "subreddit_list = [sub for sub in subreddit_list if sub in df.subreddit.unique()]\n",
    "subreddit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddits and codes added: {'css': 0, 'html': 1, 'javascript': 2, 'datascience': 3, 'machinelearning': 4, 'etl': 5, 'python': 6, 'dataengineering': 7}\n"
     ]
    }
   ],
   "source": [
    "df = dataloader.subreddit_encoder(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>date</th>\n",
       "      <th>sub_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8872</th>\n",
       "      <td>Need urgent help with a small project</td>\n",
       "      <td>html</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>International Students beware of Data Science ...</td>\n",
       "      <td>datascience</td>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9402</th>\n",
       "      <td>About coding the “FizzBuzz” interview question</td>\n",
       "      <td>javascript</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>Glowing Nav on hover</td>\n",
       "      <td>css</td>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6127</th>\n",
       "      <td>Pyspark - how do I use groupby with lists?</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>Everything You Need to Know About Regular Expr...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11374</th>\n",
       "      <td>[R] A Road Map to Strong Intelligence</td>\n",
       "      <td>machinelearning</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5560</th>\n",
       "      <td>How important is it to follow PEP guidelines f...</td>\n",
       "      <td>python</td>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>What are the limitations of css grid in terms ...</td>\n",
       "      <td>css</td>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8128</th>\n",
       "      <td>cart code</td>\n",
       "      <td>html</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title        subreddit  \\\n",
       "8872               Need urgent help with a small project             html   \n",
       "3378   International Students beware of Data Science ...      datascience   \n",
       "9402      About coding the “FizzBuzz” interview question       javascript   \n",
       "529                                 Glowing Nav on hover              css   \n",
       "6127          Pyspark - how do I use groupby with lists?  dataengineering   \n",
       "2043   Everything You Need to Know About Regular Expr...       javascript   \n",
       "11374              [R] A Road Map to Strong Intelligence  machinelearning   \n",
       "5560   How important is it to follow PEP guidelines f...           python   \n",
       "384    What are the limitations of css grid in terms ...              css   \n",
       "8128                                           cart code             html   \n",
       "\n",
       "             date  sub_code  \n",
       "8872   2020-04-02         1  \n",
       "3378   2020-03-29         3  \n",
       "9402   2020-04-02         2  \n",
       "529    2020-03-29         0  \n",
       "6127   2020-03-29         7  \n",
       "2043   2020-03-29         2  \n",
       "11374  2020-04-02         4  \n",
       "5560   2020-03-29         6  \n",
       "384    2020-03-29         0  \n",
       "8128   2020-04-02         1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "y = df['sub_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_words = set(['using', 'help', 'new', 'data', 'science', 'machine', 'learning', 'use', 'need'])\n",
    "\n",
    "custom_stop_words = ENGLISH_STOP_WORDS.union(subreddit_list, useless_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "redfuncs = Reddit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessors = grid_models.preprocessors\n",
    "estimators = grid_models.estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessors['count_vec']['pipe_params']['count_vec__stop_words'].append(custom_stop_words)\n",
    "# preprocessors['count_vec']['pipe_params']['count_vec__stop_words'].remove('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessors['tfidf']['pipe_params']['tfidf__stop_words'].append(custom_stop_words)\n",
    "# preprocessors['tfidf']['pipe_params']['tfidf__stop_words'].remove('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with CountVectorizer and Logistic Regression\n",
      "Fitting 3 folds for each of 576 candidates, totalling 1728 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   57.8s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1728 out of 1728 | elapsed:  7.1min finished\n",
      "/home/datapointchris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/datapointchris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with TfidVectorizer and Logistic Regression\n",
      "Fitting 3 folds for each of 512 candidates, totalling 1536 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   15.1s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   34.8s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1536 out of 1536 | elapsed:  2.5min finished\n",
      "/home/datapointchris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with CountVectorizer and Random Forest\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "/home/datapointchris/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   41.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 324 out of 324 | elapsed:  3.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with TfidVectorizer and Random Forest\n",
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "/home/datapointchris/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 288 out of 288 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with CountVectorizer and K Nearest Neighbors\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with TfidVectorizer and K Nearest Neighbors\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:  9.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with CountVectorizer and Multinomial Bayes Classifier\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:    8.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with TfidVectorizer and Multinomial Bayes Classifier\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:    5.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with CountVectorizer and Support Vector Classifier\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:  6.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with TfidVectorizer and Support Vector Classifier\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with CountVectorizer and AdaBoost Classifier\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done 243 out of 243 | elapsed:   18.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with TfidVectorizer and AdaBoost Classifier\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed:    9.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with CountVectorizer and Bagging Classifier\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.4min\n"
     ]
    }
   ],
   "source": [
    "compare_df = redfuncs.compare_models(X_train, X_test, y_train, y_test, cv=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare_df.sort_values(by='best_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = str(datetime.datetime.now())\n",
    "compare_df.to_csv(f'data/compare_df/{date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [pprint(params) for params in compare_df.sort_values(by='best_test_score', ascending=False)['best_params']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model = compare_df.sort_values(by='best_test_score', ascending=False).iloc[0, :].to_dict()\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a new model with the best params from the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe = Pipeline([\n",
    "    (best_model['prep_code'], preprocessors[best_model['prep_code']]['processor']),\n",
    "    (best_model['est_code'], estimators[best_model['est_code']]['estimator'])\n",
    "])\n",
    "best_pipe.set_params(**best_model['best_params'])\n",
    "# fit on entire dataset\n",
    "best_pipe.fit(X, y)\n",
    "best_pipe.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_score = cross_val_score(best_pipe, X, y)\n",
    "print(cross_score, cross_score.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much improvement over baseline\n",
    "best_pipe_score - y.value_counts(normalize=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much difference from the best worst model to the best best model\n",
    "best_pipe_score - min(compare_df['Best Test Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much improvement from retraining on entire dataset\n",
    "best_pipe_score - best_model['Best Test Score']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
