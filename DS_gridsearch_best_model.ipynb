{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Workflow\n",
    "## Find the Best Model\n",
    "\n",
    "This notebook shows how to use some of the functions located in `reddit_functions` to compare the performance of different models on the data.\n",
    "\n",
    "A second workflow is included to take the parameters of the best model and create a new model and fit it on the entire dataset and see the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helpers import databases\n",
    "from helpers import dataloader\n",
    "from helpers import grid_models\n",
    "from helpers.reddit_functions import Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_list = ['css', 'html', 'javascript', 'datascience', 'machinelearning', 'etl', 'python', 'dataengineering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subreddit_list = ['datascience','machinelearning','dataengineering','python','aws','sql']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to SQLite DB successful\n"
     ]
    }
   ],
   "source": [
    "df = dataloader.data_selector(subreddit_list, 'sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['css',\n",
       " 'html',\n",
       " 'javascript',\n",
       " 'datascience',\n",
       " 'machinelearning',\n",
       " 'etl',\n",
       " 'python',\n",
       " 'dataengineering']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get rid of list items with no data retrieved\n",
    "subreddit_list = [sub for sub in subreddit_list if sub in df.subreddit.unique()]\n",
    "subreddit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddits and codes added: {'css': 0, 'html': 1, 'javascript': 2, 'datascience': 3, 'machinelearning': 4, 'etl': 5, 'python': 6, 'dataengineering': 7}\n"
     ]
    }
   ],
   "source": [
    "df = dataloader.subreddit_encoder(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>date</th>\n",
       "      <th>sub_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>[R][P] Online Advanced Machine Learning Study ...</td>\n",
       "      <td>machinelearning</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>HELP!!! Does anyone know how to fix it?</td>\n",
       "      <td>html</td>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10763</th>\n",
       "      <td>[P] We are looking to detect hate speech in tw...</td>\n",
       "      <td>machinelearning</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10257</th>\n",
       "      <td>Can anyone tell me where to find live and hist...</td>\n",
       "      <td>datascience</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9533</th>\n",
       "      <td>VS Code Extension for Base Web and React View ...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9213</th>\n",
       "      <td>Dark Reader is now available as a JavaScript l...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3964</th>\n",
       "      <td>Transfer learning paper help [Project]</td>\n",
       "      <td>machinelearning</td>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10469</th>\n",
       "      <td>What machine learning models have you created ...</td>\n",
       "      <td>datascience</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8530</th>\n",
       "      <td>CSS var() animation redifiner using javascript</td>\n",
       "      <td>html</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7186</th>\n",
       "      <td>My pure CSS morph effect, what do you think</td>\n",
       "      <td>css</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title        subreddit  \\\n",
       "10873  [R][P] Online Advanced Machine Learning Study ...  machinelearning   \n",
       "1591             HELP!!! Does anyone know how to fix it?             html   \n",
       "10763  [P] We are looking to detect hate speech in tw...  machinelearning   \n",
       "10257  Can anyone tell me where to find live and hist...      datascience   \n",
       "9533   VS Code Extension for Base Web and React View ...       javascript   \n",
       "9213   Dark Reader is now available as a JavaScript l...       javascript   \n",
       "3964              Transfer learning paper help [Project]  machinelearning   \n",
       "10469  What machine learning models have you created ...      datascience   \n",
       "8530      CSS var() animation redifiner using javascript             html   \n",
       "7186         My pure CSS morph effect, what do you think              css   \n",
       "\n",
       "             date  sub_code  \n",
       "10873  2020-04-02         4  \n",
       "1591   2020-03-29         1  \n",
       "10763  2020-04-02         4  \n",
       "10257  2020-04-02         3  \n",
       "9533   2020-04-02         2  \n",
       "9213   2020-04-02         2  \n",
       "3964   2020-03-29         4  \n",
       "10469  2020-04-02         3  \n",
       "8530   2020-04-02         1  \n",
       "7186   2020-04-02         0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "y = df['sub_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_words = set(['using', 'help', 'new', 'data', 'science', 'machine', 'learning', 'use', 'need'])\n",
    "\n",
    "custom_stop_words = ENGLISH_STOP_WORDS.union(subreddit_list, useless_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "redfuncs = Reddit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessors = grid_models.preprocessors\n",
    "estimators = grid_models.estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessors['count_vec']['pipe_params']['count_vec__stop_words'].append(custom_stop_words)\n",
    "# preprocessors['count_vec']['pipe_params']['count_vec__stop_words'].remove('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessors['tfidf']['pipe_params']['tfidf__stop_words'].append(custom_stop_words)\n",
    "# preprocessors['tfidf']['pipe_params']['tfidf__stop_words'].remove('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with CountVectorizer and Extra Trees Classifier\n",
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "/home/datapointchris/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed: 11.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with TfidVectorizer and Extra Trees Classifier\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "/home/datapointchris/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed: 12.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with CountVectorizer and Gradient Boosting Classifier\n",
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed: 11.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with TfidVectorizer and Gradient Boosting Classifier\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed: 21.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with CountVectorizer and ElasticNet Classifier\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:    3.3s finished\n",
      "/home/datapointchris/github/reddit_nlp/helpers/reddit_functions.py:79: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  (train_score - test_score) / train_score * 100,\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with TfidVectorizer and ElasticNet Classifier\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:    2.9s finished\n",
      "/home/datapointchris/github/reddit_nlp/helpers/reddit_functions.py:79: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  (train_score - test_score) / train_score * 100,\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with CountVectorizer and Passive Agressive Classifier\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:    2.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with TfidVectorizer and Passive Agressive Classifier\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with CountVectorizer and Stochastic Gradient Descent Classifier\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:    6.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with TfidVectorizer and Stochastic Gradient Descent Classifier\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:    4.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with CountVectorizer and Nu Support Vector Classifier\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:  2.1min finished\n",
      "/home/datapointchris/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with TfidVectorizer and Nu Support Vector Classifier\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:  1.8min finished\n",
      "/home/datapointchris/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "compare_df = redfuncs.compare_models(X_train, X_test, y_train, y_test, cv=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>preprocessor</th>\n",
       "      <th>estimator</th>\n",
       "      <th>best_params</th>\n",
       "      <th>best_train_score</th>\n",
       "      <th>best_test_score</th>\n",
       "      <th>variance</th>\n",
       "      <th>prep_code</th>\n",
       "      <th>est_code</th>\n",
       "      <th>sub_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-03 20:49:46.042858</td>\n",
       "      <td>TfidVectorizer</td>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>{'extratrees__bootstrap': True, 'extratrees__c...</td>\n",
       "      <td>0.991015</td>\n",
       "      <td>0.878154</td>\n",
       "      <td>11.388467</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>extratrees</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-03 21:24:23.174866</td>\n",
       "      <td>TfidVectorizer</td>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>{'gradboost__learning_rate': 0.1, 'gradboost__...</td>\n",
       "      <td>0.985089</td>\n",
       "      <td>0.862672</td>\n",
       "      <td>12.426988</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>gradboost</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-03 20:36:23.190340</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>{'count_vec__max_df': 0.4, 'count_vec__max_fea...</td>\n",
       "      <td>0.986523</td>\n",
       "      <td>0.861525</td>\n",
       "      <td>12.670507</td>\n",
       "      <td>count_vec</td>\n",
       "      <td>extratrees</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-04-03 21:24:32.781211</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>Passive Agressive Classifier</td>\n",
       "      <td>{'count_vec__max_df': 0.5, 'count_vec__max_fea...</td>\n",
       "      <td>0.978589</td>\n",
       "      <td>0.842317</td>\n",
       "      <td>13.925421</td>\n",
       "      <td>count_vec</td>\n",
       "      <td>passive</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-03 21:01:33.374847</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>{'count_vec__max_df': 0.3, 'count_vec__max_fea...</td>\n",
       "      <td>0.961671</td>\n",
       "      <td>0.834862</td>\n",
       "      <td>13.186261</td>\n",
       "      <td>count_vec</td>\n",
       "      <td>gradboost</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-04-03 21:24:34.843634</td>\n",
       "      <td>TfidVectorizer</td>\n",
       "      <td>Passive Agressive Classifier</td>\n",
       "      <td>{'passive__C': 1.0, 'passive__average': False,...</td>\n",
       "      <td>0.967597</td>\n",
       "      <td>0.832282</td>\n",
       "      <td>13.984635</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>passive</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-04-03 21:24:41.879519</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>Stochastic Gradient Descent Classifier</td>\n",
       "      <td>{'count_vec__max_df': 0.4, 'count_vec__max_fea...</td>\n",
       "      <td>0.944944</td>\n",
       "      <td>0.810780</td>\n",
       "      <td>14.198074</td>\n",
       "      <td>count_vec</td>\n",
       "      <td>sgd</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-04-03 21:24:46.356070</td>\n",
       "      <td>TfidVectorizer</td>\n",
       "      <td>Stochastic Gradient Descent Classifier</td>\n",
       "      <td>{'sgd__alpha': 0.0001, 'sgd__average': False, ...</td>\n",
       "      <td>0.917320</td>\n",
       "      <td>0.790711</td>\n",
       "      <td>13.802036</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>sgd</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-04-03 21:29:11.756375</td>\n",
       "      <td>TfidVectorizer</td>\n",
       "      <td>Nu Support Vector Classifier</td>\n",
       "      <td>{'nusvc__cache_size': 200, 'nusvc__decision_fu...</td>\n",
       "      <td>0.822023</td>\n",
       "      <td>0.736525</td>\n",
       "      <td>10.400849</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>nusvc</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-04-03 21:27:07.690591</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>Nu Support Vector Classifier</td>\n",
       "      <td>{'count_vec__max_df': 0.3, 'count_vec__max_fea...</td>\n",
       "      <td>0.789906</td>\n",
       "      <td>0.707282</td>\n",
       "      <td>10.460002</td>\n",
       "      <td>count_vec</td>\n",
       "      <td>nusvc</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-03 21:24:26.885784</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>ElasticNet Classifier</td>\n",
       "      <td>{'count_vec__max_df': 0.3, 'count_vec__max_fea...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000339</td>\n",
       "      <td>inf</td>\n",
       "      <td>count_vec</td>\n",
       "      <td>elastic</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-04-03 21:24:30.068496</td>\n",
       "      <td>TfidVectorizer</td>\n",
       "      <td>ElasticNet Classifier</td>\n",
       "      <td>{'elastic__alpha': 1.0, 'elastic__copy_X': Tru...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000339</td>\n",
       "      <td>inf</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>elastic</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date     preprocessor  \\\n",
       "1  2020-04-03 20:49:46.042858   TfidVectorizer   \n",
       "3  2020-04-03 21:24:23.174866   TfidVectorizer   \n",
       "0  2020-04-03 20:36:23.190340  CountVectorizer   \n",
       "6  2020-04-03 21:24:32.781211  CountVectorizer   \n",
       "2  2020-04-03 21:01:33.374847  CountVectorizer   \n",
       "7  2020-04-03 21:24:34.843634   TfidVectorizer   \n",
       "8  2020-04-03 21:24:41.879519  CountVectorizer   \n",
       "9  2020-04-03 21:24:46.356070   TfidVectorizer   \n",
       "11 2020-04-03 21:29:11.756375   TfidVectorizer   \n",
       "10 2020-04-03 21:27:07.690591  CountVectorizer   \n",
       "4  2020-04-03 21:24:26.885784  CountVectorizer   \n",
       "5  2020-04-03 21:24:30.068496   TfidVectorizer   \n",
       "\n",
       "                                 estimator  \\\n",
       "1                   Extra Trees Classifier   \n",
       "3             Gradient Boosting Classifier   \n",
       "0                   Extra Trees Classifier   \n",
       "6             Passive Agressive Classifier   \n",
       "2             Gradient Boosting Classifier   \n",
       "7             Passive Agressive Classifier   \n",
       "8   Stochastic Gradient Descent Classifier   \n",
       "9   Stochastic Gradient Descent Classifier   \n",
       "11            Nu Support Vector Classifier   \n",
       "10            Nu Support Vector Classifier   \n",
       "4                    ElasticNet Classifier   \n",
       "5                    ElasticNet Classifier   \n",
       "\n",
       "                                          best_params  best_train_score  \\\n",
       "1   {'extratrees__bootstrap': True, 'extratrees__c...          0.991015   \n",
       "3   {'gradboost__learning_rate': 0.1, 'gradboost__...          0.985089   \n",
       "0   {'count_vec__max_df': 0.4, 'count_vec__max_fea...          0.986523   \n",
       "6   {'count_vec__max_df': 0.5, 'count_vec__max_fea...          0.978589   \n",
       "2   {'count_vec__max_df': 0.3, 'count_vec__max_fea...          0.961671   \n",
       "7   {'passive__C': 1.0, 'passive__average': False,...          0.967597   \n",
       "8   {'count_vec__max_df': 0.4, 'count_vec__max_fea...          0.944944   \n",
       "9   {'sgd__alpha': 0.0001, 'sgd__average': False, ...          0.917320   \n",
       "11  {'nusvc__cache_size': 200, 'nusvc__decision_fu...          0.822023   \n",
       "10  {'count_vec__max_df': 0.3, 'count_vec__max_fea...          0.789906   \n",
       "4   {'count_vec__max_df': 0.3, 'count_vec__max_fea...          0.000000   \n",
       "5   {'elastic__alpha': 1.0, 'elastic__copy_X': Tru...          0.000000   \n",
       "\n",
       "    best_test_score   variance  prep_code    est_code sub_list  \n",
       "1          0.878154  11.388467      tfidf  extratrees       na  \n",
       "3          0.862672  12.426988      tfidf   gradboost       na  \n",
       "0          0.861525  12.670507  count_vec  extratrees       na  \n",
       "6          0.842317  13.925421  count_vec     passive       na  \n",
       "2          0.834862  13.186261  count_vec   gradboost       na  \n",
       "7          0.832282  13.984635      tfidf     passive       na  \n",
       "8          0.810780  14.198074  count_vec         sgd       na  \n",
       "9          0.790711  13.802036      tfidf         sgd       na  \n",
       "11         0.736525  10.400849      tfidf       nusvc       na  \n",
       "10         0.707282  10.460002  count_vec       nusvc       na  \n",
       "4         -0.000339        inf  count_vec     elastic       na  \n",
       "5         -0.000339        inf      tfidf     elastic       na  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_df.sort_values(by='best_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/datapointchris/github/reddit_nlp\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date = str(datetime.datetime.now())\n",
    "compare_df.to_csv(f'data/compare_df/{date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [pprint(params) for params in compare_df.sort_values(by='best_test_score', ascending=False)['best_params']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': Timestamp('2020-04-03 20:49:46.042858'),\n",
       " 'preprocessor': 'TfidVectorizer',\n",
       " 'estimator': 'Extra Trees Classifier',\n",
       " 'best_params': {'extratrees__bootstrap': True,\n",
       "  'extratrees__class_weight': None,\n",
       "  'extratrees__max_depth': None,\n",
       "  'extratrees__max_leaf_nodes': None,\n",
       "  'extratrees__min_samples_leaf': 1,\n",
       "  'extratrees__min_samples_split': 2,\n",
       "  'extratrees__min_weight_fraction_leaf': 0.0,\n",
       "  'extratrees__n_estimators': 500,\n",
       "  'tfidf__max_features': 5000,\n",
       "  'tfidf__ngram_range': (1, 1),\n",
       "  'tfidf__norm': 'l1',\n",
       "  'tfidf__stop_words': 'english',\n",
       "  'tfidf__strip_accents': None,\n",
       "  'tfidf__use_idf': True},\n",
       " 'best_train_score': 0.9910151022748996,\n",
       " 'best_test_score': 0.8781536697247706,\n",
       " 'variance': 11.388467470480807,\n",
       " 'prep_code': 'tfidf',\n",
       " 'est_code': 'extratrees',\n",
       " 'sub_list': 'na'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = compare_df.sort_values(by='best_test_score', ascending=False).iloc[0, :].to_dict()\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a new model with the best params from the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9886021505376344"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipe = Pipeline([\n",
    "    (best_model['prep_code'], preprocessors[best_model['prep_code']]['processor']),\n",
    "    (best_model['est_code'], estimators[best_model['est_code']]['estimator'])\n",
    "])\n",
    "best_pipe.set_params(**best_model['best_params'])\n",
    "# fit on entire dataset\n",
    "best_pipe.fit(X, y)\n",
    "best_pipe_score = best_pipe.score(X, y)\n",
    "best_pipe_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datapointchris/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98151333 0.89591398 0.98881239] 0.9554132328408352\n"
     ]
    }
   ],
   "source": [
    "cross_score = cross_val_score(best_pipe, X, y)\n",
    "print(cross_score, cross_score.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    0.142437\n",
       "7    0.140000\n",
       "1    0.139140\n",
       "2    0.133692\n",
       "6    0.132760\n",
       "0    0.131971\n",
       "3    0.126237\n",
       "5    0.053763\n",
       "Name: sub_code, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8566308243727598"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how much improvement over baseline\n",
    "best_pipe_score - y.value_counts(normalize=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9889406777941373"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how much difference from the best worst model to the best best model\n",
    "best_pipe_score - min(compare_df['best_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11044848081286385"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how much improvement from retraining on entire dataset\n",
    "best_pipe_score - best_model['best_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
