{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from PIL import Image\n",
    "import wordcloud\n",
    "import time\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from glob import glob\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chris/github/reddit_nlp\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from util import databases\n",
    "from util import dataloader\n",
    "from util import grid_models\n",
    "from util.reddit_functions import Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_list = ['datascience','machinelearning','dataengineering','python','aws']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to SQLite DB successful\n"
     ]
    }
   ],
   "source": [
    "df = dataloader.data_selector(subreddit_list, 'sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datascience', 'machinelearning', 'dataengineering', 'python', 'aws']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get rid of list items with no data retrieved\n",
    "subreddit_list = [sub for sub in subreddit_list if sub in df.subreddit.unique()]\n",
    "subreddit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>Feeling overwhelmed at mid-life career change ...</td>\n",
       "      <td>datascience</td>\n",
       "      <td>2020-03-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7567</th>\n",
       "      <td>[P] Train a Facial Recognition model from scratch</td>\n",
       "      <td>machinelearning</td>\n",
       "      <td>2020-04-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4258</th>\n",
       "      <td>Interview Questions</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>2020-03-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3712</th>\n",
       "      <td>Is it bad practice to use function that are on...</td>\n",
       "      <td>python</td>\n",
       "      <td>2020-03-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963</th>\n",
       "      <td>Anybody here switch from data scientist to dat...</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>2020-03-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title        subreddit  \\\n",
       "1520  Feeling overwhelmed at mid-life career change ...      datascience   \n",
       "7567  [P] Train a Facial Recognition model from scratch  machinelearning   \n",
       "4258                                Interview Questions  dataengineering   \n",
       "3712  Is it bad practice to use function that are on...           python   \n",
       "3963  Anybody here switch from data scientist to dat...  dataengineering   \n",
       "\n",
       "            date  \n",
       "1520  2020-03-29  \n",
       "7567  2020-04-02  \n",
       "4258  2020-03-29  \n",
       "3712  2020-03-29  \n",
       "3963  2020-03-29  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder().fit(y)\n",
    "y = label_encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_words = set(['using', 'lambda', 's3', 'does', 'looking', 'help', 'new', 'data', 'science', 'machine', 'learning', 'use', 'need', 'engineer', 'engineering'])\n",
    "\n",
    "custom_stop_words = ENGLISH_STOP_WORDS.union(subreddit_list, useless_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.7, max_features=6000, ngram_range=(1,2), stop_words=custom_stop_words)\n",
    "svc = SVC()\n",
    "pipe = Pipeline([('tfidf', tfidf),('svc', svc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since GridSearch does automatic refit on the entire training set, we can use the transformer from this best model to make a features df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8424801005446166"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIX ###\n",
    "# Hardcoded count_vec\n",
    "# Test for tfidf, may have to hardcode into ds_workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vizualizer:\n",
    "    \"\"\"\n",
    "    Produces visualizations for a fitted model\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "vect",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'vect'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-520c705894f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfeatures_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mfeatures_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfeatures_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: vect"
     ]
    }
   ],
   "source": [
    "# model is GridSearchCV\n",
    "# best_estimator is Pipeline\n",
    "# named_steps is the steps in the pipeline\n",
    "# count_vec is the FITTED \n",
    "\n",
    "def create_features_df():\n",
    "    \"\"\"\n",
    "    Creates features df from model\n",
    "    \"\"\"\n",
    "\n",
    "    features_data = model.best_estimator_.named_steps.vect.transform(X).toarray()\n",
    "    features_columns = model.best_estimator_.named_steps.vect.get_feature_names()\n",
    "    features_df = pd.DataFrame(data=features_data, columns=features_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK ### does this function work with tfidf, or just count vec?\n",
    "def plot_most_common(df=df, features_df=features_df, subreddit_list=subreddit_list, num_features=20, standardize=False, include_combined=False):\n",
    "    '''\n",
    "    Plots the most common features for each subreddit in the DataFrame\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    df: original DataFrame\n",
    "\n",
    "    features_df: should be output from transformer on df\n",
    "\n",
    "        Example:\n",
    "        features_df = pd.DataFrame(\n",
    "                                data={transformer}.transform(X).toarray(),\n",
    "                                columns={transformer}.get_feature_names())\n",
    "\n",
    "    num_features: number of most common features to plot for each subreddit\n",
    "\n",
    "    standardize: put all of the plots on the same scale\n",
    "\n",
    "    combined: include a plot of the most common features of all of the subreddits combined\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    plots\n",
    "\n",
    "    '''\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=1,\n",
    "                           nrows=len(subreddit_list) + int(1 if include_combined else 0),\n",
    "                           figsize=(15, num_features/1.3*len(subreddit_list)))\n",
    "\n",
    "    for subplot_idx, sub in enumerate(subreddit_list):\n",
    "        sub_features = features_df.loc[df['subreddit'] == sub]\n",
    "        sub_top_words = sub_features.sum().sort_values(ascending=False).head(num_features)[::-1]\n",
    "        sub_top_words.plot(kind='barh', ax=ax[subplot_idx])\n",
    "        ax[subplot_idx].set_title(f'{num_features} Most Common Words for {sub.upper()}', fontsize=16)\n",
    "        \n",
    "        if standardize:\n",
    "            max_occurence = features_df.sum().max()*1.02\n",
    "            ax[subplot_idx].set_xlim(0, max_occurence)\n",
    "\n",
    "    if include_combined:\n",
    "        most_common = features_df.sum().sort_values(ascending=False).head(num_features)[::-1]\n",
    "        most_common.plot(kind='barh', ax=ax[subplot_idx+1])\n",
    "        ax[subplot_idx+1].set_title(f'{num_features} Most Common Words for ({\", \".join(subreddit_list).upper()})')\n",
    "        \n",
    "        if standardize:\n",
    "            ax[subplot_idx+1].set_xlim(0, max_occurence)\n",
    "    \n",
    "    plt.tight_layout(h_pad=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_most_common(df, features_df, num_features=15, include_combined=True, standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###HELP### I don't think this is working right\n",
    "###HELP### coefficients don't make sense for the entire dataset, would need to do one for each thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###NOTE### coefficients only for lr and etc etc etc...\n",
    "# ###NOTE### coefficients only for two subreddits\n",
    "\n",
    "# ### FIX ### look at the coef_ portion of the new single model instead of the gridsearch\n",
    "\n",
    "\n",
    "# # [-1][1] for last step (estimator)(instantiation)\n",
    "# # coef_[0]because I don't know why\n",
    "# coefs = model.best_estimator_.steps[-1][1].coef_[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(16, 12))\n",
    "# plt.title('Feature Importance (Coefficients)', fontsize=20)\n",
    "# plt.ylabel('Features', fontsize=18)\n",
    "# plt.xlabel('(Abs) Coefficients', fontsize=18)\n",
    "\n",
    "# coef_df = pd.DataFrame(data=[coefs], columns=features_columns).T\n",
    "# coef_df['abs_coef'] = coef_df[0].abs()\n",
    "# coef_df.sort_values('abs_coef', ascending=False)[0].head(15).plot(kind='barh');\n",
    "\n",
    "# # coef_kill = coef_df.sort_values('abs_coef', ascending=False)[0].head(500).index\n",
    "\n",
    "# # coef_kill\n",
    "\n",
    "\n",
    "# # my_stops.extend(coef_kill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(subreddit_list):\n",
    "    '''\n",
    "    Makes combination pairs of subreddits from subreddit_list\n",
    "    '''\n",
    "    if len(subreddit_list) > 2:\n",
    "            return list(combinations(subreddit_list, 2))\n",
    "    return subreddit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = make_pairs(subreddit_list)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_common_pairs(df, features_df, pairs, num_features=20):\n",
    "    '''\n",
    "    Plots the most common features for each subreddit in the DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    df: original DataFrame\n",
    "    \n",
    "    features_df: should be output from transformer on df\n",
    "        \n",
    "        Example:\n",
    "        features_df = pd.DataFrame(\n",
    "                                data={transformer}.transform(X).toarray(),\n",
    "                                columns={transformer}.get_feature_names())\n",
    "    \n",
    "    num_features: number of most common features to plot for each subreddit\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    plots\n",
    "    \n",
    "    '''\n",
    "    fig, ax = plt.subplots(ncols=2, \n",
    "                           nrows=len(pairs), \n",
    "                           figsize=(16,num_features/3*len(pairs)))\n",
    "\n",
    "    for i, pair in enumerate(pairs):\n",
    "\n",
    "        # features for each pair\n",
    "        feats_0 = features_df.loc[(df['subreddit'] == pair[0])]\n",
    "        feats_1 = features_df.loc[(df['subreddit'] == pair[1])]\n",
    "        # combined\n",
    "        common_feats = feats_0.append(feats_1)\n",
    "        # this is the most common between the two\n",
    "        most_common = common_feats.sum().sort_values(ascending=False).head(num_features)[::-1]\n",
    "        # plot\n",
    "        feats_0[most_common.index].sum().plot.barh(ax=ax[i, 0], color='navy')\n",
    "        feats_1[most_common.index].sum().plot.barh(ax=ax[i, 1], color='orange')\n",
    "        ax[i, 0].set_title(f'Top {num_features} - {pair} \\nSub: {pair[0].upper()}', fontsize=16, wrap=True)\n",
    "        ax[i, 1].set_title(f'Top {num_features} - {pair} \\nSub: {pair[1].upper()}', fontsize=16, wrap=True)\n",
    "        max_occurence = common_feats.sum().max()*1.02\n",
    "        ax[i, 0].set_xlim(0,max_occurence)\n",
    "        ax[i, 1].set_xlim(0,max_occurence)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_most_common_pairs(df, features_df, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = features_df.sum().sort_values(ascending=False).head(20)[::-1]\n",
    "groups = features_df.groupby(df['subreddit']).sum()[most_common.index].T.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, figsize=(18,20))\n",
    "\n",
    "groups.plot.bar(ax=ax[0], width=.8, fontsize=15)\n",
    "ax[0].set_title('20 Most Common Words', fontsize=20)\n",
    "ax[0].set_ylabel('# of Occurences', fontsize=15)\n",
    "ax[0].legend(fontsize=15, fancybox=True, framealpha=1, shadow=True, borderpad=1)\n",
    "\n",
    "groups.plot(kind='bar', ax=ax[1], width=.35, fontsize=15, stacked=True)\n",
    "ax[1].set_title('20 Most Common Words', fontsize=20)\n",
    "ax[1].set_ylabel('# of Occurences', fontsize=15)\n",
    "ax[1].legend(fontsize=15, fancybox=True, framealpha=1, shadow=True, borderpad=1)\n",
    "\n",
    "\n",
    "plt.tight_layout(h_pad=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(cm, columns=subreddit_list, index=subreddit_list)\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "df_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "sns.set(font_scale=2)\n",
    "sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, cmap='Greens', ax=ax, )\n",
    "fontdict={'fontsize': 16}\n",
    "ax.set_yticklabels(labels=subreddit_list, rotation='horizontal', fontdict=fontdict)\n",
    "ax.set_xticklabels(labels=subreddit_list, rotation=20, fontdict=fontdict)\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position('top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "mtn = mcm[:, 0, 0]\n",
    "mtp = mcm[:, 1, 1]\n",
    "mfn = mcm[:, 1, 0]\n",
    "mfp = mcm[:, 0, 1]\n",
    "print(mcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=len(subreddit_list),\n",
    "                       figsize=(12, 6*len(subreddit_list)))\n",
    "\n",
    "for i, cm in enumerate(mcm):\n",
    "    df_cm = pd.DataFrame(cm)\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted'\n",
    "    sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False,\n",
    "                cmap='Purples', ax=ax[i, 0])\n",
    "\n",
    "    ax[i, 0].set_yticklabels(labels=[1, 0], rotation='horizontal')\n",
    "    ax[i, 0].set_xticklabels(labels=[1, 0])\n",
    "    ax[i, 0].xaxis.tick_top()\n",
    "    ax[i, 0].xaxis.set_label_position('top')\n",
    "    ax[i, 0].set_title(subreddit_list[i].upper())\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specif = tn / (tn + fp)\n",
    "\n",
    "    sens = tp / (tp + fn)\n",
    "    box_text = f'''Subreddit: {subreddit_list[i].upper()}\\n\\nSpecificity: {round(specif,4)}\\n\\nSensitivity: {round(sens,4)}'''\n",
    "    ax[i, 1].text(0.5, 0.5, box_text, horizontalalignment='center',\n",
    "                  verticalalignment='center', fontsize=24)\n",
    "    ax[i, 1].set_axis_off()\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, digits=3, target_names=subreddit_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TODO:</h1>\n",
    "\n",
    "1. Make it have the roc for each sub, have to get into the original df where subname equals indexes?\n",
    "2. Plot confusion matrix\n",
    "3. Make a notebook to test the confusion matrixes one by one with each individual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y_test, y_pred)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not multiclass\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (area = {auc})', color='r', marker='D')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', size=16)\n",
    "plt.ylabel('True Positive Rate (Sensitivity)', size=16)\n",
    "plt.title('ROC Curve', size=20)\n",
    "plt.legend(fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.normal(0, 0.8, 1000) x2 = np.random.normal(-2, 1, 1000) x3 = np.random.normal(3, 2, 1000)\n",
    "kwargs = dict(histtype='stepfilled', alpha=0.3, normed=True, bins=40)\n",
    "plt.hist(x1, **kwargs) plt.hist(x2, **kwargs) plt.hist(x3, **kwargs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create histogram of observations.\n",
    "plt.hist(probs, bins=25, color='b')\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Distribution of P(Outcome = 1)', fontsize=22)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xlabel('Predicted Probability that Outcome = 1', fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create two histograms of observations.\n",
    "hst0 = plt.hist(probs[y_test == 0],\n",
    "         bins=25,\n",
    "         color='b',\n",
    "         alpha = 0.6,\n",
    "         label='Technology',)\n",
    "\n",
    "hst1 = plt.hist(probs[y_test == 1],\n",
    "         bins=25,\n",
    "         color='orange',\n",
    "         alpha = 0.6,\n",
    "         label='Science')\n",
    "\n",
    "# Add vertical line at P(Outcome = 1) = 0.5.\n",
    "plt.vlines(x=0.5,\n",
    "           ymin = 0,\n",
    "           ymax = max(hst1[0].max(), hst0[0].max()), # Max of the two highest respective hist values\n",
    "           color='r',\n",
    "           linestyle = '--')\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Distribution of P(Science)', fontsize=22)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xlabel('Predicted Probability', fontsize=18)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=20);\n",
    "\n",
    "# Thanks to Matt Brems for the colorful graphs! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
