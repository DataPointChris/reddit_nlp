{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from time import sleep\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import wordcloud\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # shhhhhhhh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Model Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary of Models and Preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessors\n",
    "\n",
    "count_vec = {\n",
    "    'name': 'CountVectorizer',\n",
    "    'abbr': 'count_vec',\n",
    "    'processor': CountVectorizer(),\n",
    "    'pipe_params' : {\n",
    "                'count_vec__max_features': [5000],\n",
    "                'count_vec__max_df': [.3,.4,.5],\n",
    "                'count_vec__ngram_range': [(1,2)],\n",
    "                'count_vec__stop_words': ['english'],\n",
    "                'count_vec__min_df': [4,5,6]}\n",
    "}\n",
    "\n",
    "\n",
    "tfidf = {\n",
    "    'name': 'TfidVectorizer',\n",
    "    'abbr': 'tfidf',\n",
    "    'processor': TfidfVectorizer(),\n",
    "    'pipe_params' : {\n",
    "                'tfidf__strip_accents': [None],\n",
    "                'tfidf__stop_words': ['english'],\n",
    "                'tfidf__ngram_range': [(1, 1)],                   \n",
    "                'tfidf__max_features': [5000]}\n",
    "}\n",
    "\n",
    "    \n",
    "    \n",
    "# Estimators\n",
    "    \n",
    "lr = {\n",
    "    'name': 'Logistic Regression',\n",
    "    'abbr': 'lr',\n",
    "    'estimator': LogisticRegression(),\n",
    "    'pipe_params' : {\n",
    "                'lr__penalty': ['l1','l2'],\n",
    "                'lr__C': [.01,.1,1,3]}\n",
    "}\n",
    "\n",
    "\n",
    "rf = {\n",
    "    'name': 'Random Forest',\n",
    "    'abbr': 'rf',\n",
    "    'estimator': RandomForestClassifier(),\n",
    "    'pipe_params' : {\n",
    "                'rf__n_estimators': [100, 200, 300],\n",
    "                'rf__max_depth': [200],\n",
    "                'rf__min_samples_leaf': [1,2,3],\n",
    "                'rf__min_samples_split': [.0005, .001, .01]}\n",
    "}\n",
    "\n",
    "\n",
    "knn = {\n",
    "    'name': 'K Nearest Neighbors',\n",
    "    'abbr': 'knn',\n",
    "    'estimator': KNeighborsClassifier(),\n",
    "    'pipe_params' : {\n",
    "                'knn__n_neighbors': [3,5,7],\n",
    "                'knn__metric': ['manhattan']}\n",
    "}\n",
    "\n",
    "    \n",
    "mnb = {\n",
    "    'name': 'Multinomial Bayes Classifier',\n",
    "    'abbr': 'mnb',\n",
    "    'estimator': MultinomialNB(),\n",
    "    'pipe_params' : {\n",
    "                'mnb__fit_prior': [False],\n",
    "                'mnb__alpha': [0,.1,1]}\n",
    "}\n",
    "    \n",
    "    \n",
    "svc = {\n",
    "    'name': 'Support Vector Classifier',\n",
    "    'abbr': 'svc',\n",
    "    'estimator': SVC(),\n",
    "    'pipe_params' : {\n",
    "                'svc__C': [1,2,3,4,5],\n",
    "                'svc__kernel': ['linear', 'poly','rbf'],\n",
    "                'svc__gamma': ['scale'],\n",
    "                'svc__degree': [1,2,3,4,5],\n",
    "                'svc__probability': [True]}\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists of Models and Preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [lr, rf, knn, mnb, svc]\n",
    "preprocessors = [count_vec, tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    \n",
    "    def __init__(self, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test):\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        \n",
    "###NOTE### Do train test split in DS workflow, no need to double abstract the train test split\n",
    "    \n",
    "    \n",
    "    def make_model(self, prep, est):\n",
    "\n",
    "\n",
    "     \n",
    "    \n",
    "    def scoring(self):\n",
    "        print('Best Parameters')\n",
    "        print(self.grid.best_params_)\n",
    "        print()\n",
    "        \n",
    "        print('Best Training Score:')\n",
    "        print(self.grid.score(self.X_train, self.y_train))\n",
    "        print()\n",
    "        \n",
    "        print('Best Testing Score:')\n",
    "        print(self.grid.score(self.X_test, self.y_test))\n",
    "        \n",
    "        \n",
    "    def confused(self):\n",
    "        \n",
    "        print('Confusion Matrix')\n",
    "        cm = confusion_matrix(self.y_test, self.predictions)\n",
    "\n",
    "        confusion_df = pd.DataFrame(cm, \n",
    "                                    columns=[f'Pred False: {self.subname2.capitalize()}', \n",
    "                                             f'Pred True: {self.subname1.capitalize()}'],\n",
    "                                        \n",
    "                                    index=[f'Actual False: {self.subname2.capitalize()}', \n",
    "                                           f'Actual True: {self.subname1.capitalize()}'])\n",
    "\n",
    "\n",
    "        display(confusion_df)\n",
    "\n",
    "        print()\n",
    "        print()\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        print('### Specificity')\n",
    "\n",
    "        spec = tn / (tn + fp)\n",
    "        print(spec)\n",
    "        print()\n",
    "        \n",
    "        print('### Sensitivity/Recall')\n",
    "\n",
    "        sens = tp / (tp +fn)\n",
    "        print(sens)\n",
    "        \n",
    "        \n",
    "    def hist_dist_p(self):\n",
    "        # Create figure.\n",
    "        plt.figure(figsize = (10,7))\n",
    "\n",
    "        # Create histogram of observations.\n",
    "        plt.hist(self.probs, bins=25, color='b')\n",
    "\n",
    "        # Label axes.\n",
    "        plt.title(f'Distribution of P({self.subname1.capitalize()}: 1)', fontsize=22)\n",
    "        plt.ylabel('Frequency', fontsize=18)\n",
    "        plt.xlabel(f'Predicted Probability that Outcome = 1 ({self.subname1.capitalize()})', fontsize=18);\n",
    "\n",
    "        \n",
    "        \n",
    "           \n",
    "\n",
    "        \n",
    "    def outcome_hists(self):\n",
    "        '''\n",
    "        * Thanks to Matt Brems for the outline code *\n",
    "        Function does not need to take any arguments if the model has been fit, unless a different probability or y_test is desired.\n",
    "        '''\n",
    "\n",
    "        \n",
    "            # Create figure.\n",
    "        plt.figure(figsize = (10,7))\n",
    "\n",
    "        # Create two histograms of observations.\n",
    "        hst0 = plt.hist(self.probs[self.y_test == 0],\n",
    "                 bins=25,\n",
    "                 color='b',\n",
    "                 alpha = 0.6,\n",
    "                 label=f'{self.subname2.capitalize()}: 0',)\n",
    "\n",
    "        hst1 = plt.hist(self.probs[self.y_test == 1],\n",
    "                 bins=25,\n",
    "                 color='orange',\n",
    "                 alpha = 0.6,\n",
    "                 label=f'{self.subname1.capitalize()}: 1')\n",
    "\n",
    "        # Add vertical line at P(Outcome = 1) = 0.5.\n",
    "        plt.vlines(x=0.5,\n",
    "                   ymin = 0,\n",
    "                   ymax = max(hst1[0].max(), hst0[0].max()), # Max of the two highest respective hist values\n",
    "                   color='r',\n",
    "                   linestyle = '--')\n",
    "\n",
    "        # Label axes.\n",
    "        plt.title(f'Distribution of P({self.subname1.capitalize()})', fontsize=22)\n",
    "        plt.ylabel('Frequency', fontsize=18)\n",
    "        plt.xlabel('Predicted Probability that Outcome = 1', fontsize=18)\n",
    "\n",
    "        # Create legend.\n",
    "        plt.legend(fontsize=20);\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def make_roc(self): \n",
    "        '''\n",
    "        Name as a string, probs as a variable that should be calculated already.\n",
    "        Specify y_test only if it is different\n",
    "        '''\n",
    "        this_auc = roc_auc_score(self.y_test, self.probs)\n",
    "        this_fpr, this_tpr, thresholds = roc_curve(self.y_test, self.probs)\n",
    "\n",
    "        plt.figure(figsize=(12, 9))\n",
    "        plt.plot(this_fpr, this_tpr, color='r', label=this_auc)\n",
    "        plt.legend(f'ROC curve (area = {this_auc})', loc='lower right', bbox_to_anchor=(0.5, 0., 0.5, 0.5))\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        # plt.vlines(1,0,1)\n",
    "        # plt.hlines(1,0,1)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.xlabel('False Positive Rate (1 - Specificity)', size=16)\n",
    "        plt.ylabel('True Positive Rate (Sensitivity)', size=16)\n",
    "        plt.title(f'ROC Curve: {self.est[\"name\"]}', size=20)\n",
    "        plt.legend(fontsize=14);\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def make_cloud(self, split=None):\n",
    "        '''\n",
    "        No inputs needed.  Uses X value of data, which should be text columns.\n",
    "        If you want word clouds for each subreddit, use split='y'\n",
    "        '''\n",
    "        wc = wordcloud.WordCloud(max_words=50, \n",
    "                                 width=700, \n",
    "                                 height=400, \n",
    "                                 background_color='white',\n",
    "                                )\n",
    "        if split == 'y':\n",
    "            print(f'Cloud for {self.subname1.capitalize()}')\n",
    "            cloud1 = wc.generate(self.X.loc[self.full_df['subreddit'] == 1].str.cat())\n",
    "            display(cloud1.to_image())\n",
    "            print()\n",
    "            print()\n",
    "            print(f'Cloud for {self.subname2.capitalize()}')\n",
    "            cloud0 = wc.generate(self.X.loc[self.full_df['subreddit'] == 0].str.cat())\n",
    "            display(cloud0.to_image())\n",
    "        else:\n",
    "            cloud = wc.generate(self.X.str.cat())\n",
    "            return cloud.to_image()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "    def compare_models(self, models, preprocessors):\n",
    "        self.models = models\n",
    "        self.preprocessors = preprocessors\n",
    "        \n",
    "        # Set up the dataframe for comparison with the column names          \n",
    "        self.model_comp = pd.DataFrame(columns=['Preprocessor',\n",
    "                                                'Estimator', \n",
    "                                                'Best Params', \n",
    "                                                'Best Train Score',\n",
    "                                                'Best Test Score',\n",
    "                                                'Variance'\n",
    "                                               ])         \n",
    "                  \n",
    "        # Loop through the models and preprocessors\n",
    "        #### NOTE: Need to make an if statement to not use some preprocessors with some models.\n",
    "        for model in models:\n",
    "            for process in preprocessors:\n",
    "                self.make_model(process, model)\n",
    "                  \n",
    "                self.model_comp.loc[len(self.model_comp)] = [\n",
    "                                        process['name'], \n",
    "                                        model['name'], \n",
    "                                        self.grid.best_params_, \n",
    "                                        self.grid.score(self.X_train, self.y_train), \n",
    "                                        self.grid.score(self.X_test, self.y_test),\n",
    "                                        (self.grid.score(self.X_test, self.y_test))  \n",
    "                                          - (self.grid.score(self.X_train, self.y_train))\n",
    "                                      ]\n",
    "\n",
    "        display(self.model_comp)\n",
    "                  \n",
    "                  \n",
    "                  \n",
    "                  \n",
    "# THIS DOESN'T WORK\n",
    "                  \n",
    "    def most_words(self):\n",
    "\n",
    "        feat_names = self.grid.best_estimator_.named_steps[self.prep['abbr']].get_feature_names()\n",
    "\n",
    "        catcher = self.grid.best_estimator_.named_steps[self.est['abbr']]\n",
    "\n",
    "        sparse_matrix = self.grid.best_estimator_.named_steps[self.prep['abbr']].fit_transform(self.X_test)\n",
    "\n",
    "        dense_matrix = sparse_matrix.todense()\n",
    "\n",
    "        feature_df = pd.DataFrame(dense_matrix, columns=feat_names)\n",
    "\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        feature_df.sum().sort_values(ascending=False).head(10).plot(kind='barh');\n",
    "\n",
    "                  \n",
    "\n",
    "                  \n",
    "    def do_everything(self, prep, est):\n",
    "                  '''\n",
    "                  Makes model and gives scores and distribution graphs\n",
    "                  (preprocessor, estimator)\n",
    "                  '''\n",
    "                  self.prep = prep\n",
    "                  self.est = est\n",
    "                  \n",
    "                  \n",
    "                  self.make_model(prep, est)\n",
    "                  \n",
    "                  self.scoring()\n",
    "                  \n",
    "                  self.confused()\n",
    "                  \n",
    "                  self.make_roc()\n",
    "                  \n",
    "                  self.hist_dist_p()\n",
    "                  \n",
    "                  self.outcome_hists()\n",
    "                  \n",
    "                  \n",
    "\n",
    "## BELOW IS A FUNCTION TO DROP THE TOP 'X' FEATURES FROM THE DATASET                  \n",
    "                  \n",
    "#         top_feats = feature_df.sum().sort_values(ascending=False)[:10,].index\n",
    "\n",
    "        # top_feats\n",
    "\n",
    "        # my_stops.extend(top_feats)\n",
    "\n",
    "                  \n",
    "                  \n",
    "                  \n",
    "## THIS IS A FUNCTION THAT PLOTS THE ABSOLUTE COEFFICIENTS\n",
    "                  ## VARIABLES SHOULD BE DEFINED IN SCORE, CONFUSION, OR OTHER DIST PLOT FUNCTIONS\n",
    "\n",
    "                  \n",
    "#         plt.figure(figsize=(16, 12))\n",
    "#         coef_df = pd.DataFrame(catcher.coef_, columns=feat_names).T\n",
    "#         coef_df['abs_coef'] = coef_df[0].abs()\n",
    "#         coef_df.sort_values('abs_coef', ascending=False)[0].head(15).plot(kind='barh');\n",
    "\n",
    "        # coef_kill = coef_df.sort_values('abs_coef', ascending=False)[0].head(500).index\n",
    "\n",
    "        # coef_kill\n",
    "\n",
    "\n",
    "        # my_stops.extend(coef_kill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting posts from Depression.\n",
      "We are on round 1, total of 0 posts.\n",
      "We are on round 2, total of 25 posts.\n",
      "We are on round 3, total of 50 posts.\n",
      "We are on round 4, total of 75 posts.\n",
      "We are on round 5, total of 100 posts.\n",
      "We are on round 6, total of 125 posts.\n",
      "We are on round 7, total of 150 posts.\n",
      "We are on round 8, total of 175 posts.\n",
      "We are on round 9, total of 200 posts.\n",
      "We are on round 10, total of 225 posts.\n",
      "We are on round 11, total of 250 posts.\n",
      "We are on round 12, total of 275 posts.\n",
      "We are on round 13, total of 300 posts.\n",
      "We are on round 14, total of 325 posts.\n",
      "We are on round 15, total of 350 posts.\n",
      "We are on round 16, total of 375 posts.\n",
      "We are on round 17, total of 400 posts.\n",
      "We are on round 18, total of 425 posts.\n",
      "We are on round 19, total of 450 posts.\n",
      "We are on round 20, total of 475 posts.\n",
      "We are on round 21, total of 500 posts.\n",
      "We are on round 22, total of 525 posts.\n",
      "We are on round 23, total of 550 posts.\n",
      "We are on round 24, total of 575 posts.\n",
      "We are on round 25, total of 600 posts.\n",
      "We are on round 26, total of 625 posts.\n",
      "We are on round 27, total of 650 posts.\n",
      "We are on round 28, total of 675 posts.\n",
      "We are on round 29, total of 700 posts.\n",
      "We are on round 30, total of 725 posts.\n",
      "We are on round 31, total of 750 posts.\n",
      "We are on round 32, total of 775 posts.\n",
      "We are on round 33, total of 800 posts.\n",
      "We are on round 34, total of 825 posts.\n",
      "We are on round 35, total of 850 posts.\n",
      "We are on round 36, total of 875 posts.\n",
      "We are on round 37, total of 900 posts.\n",
      "We are on round 38, total of 925 posts.\n",
      "We are on round 39, total of 950 posts.\n",
      "We are on round 40, total of 975 posts.\n",
      "Finished. 998 total posts.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NHS health care..told I don’t need further help..</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I just want to end it</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sort of a random question but how bad is it if...</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I feel nothing</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im fucking stupid</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   subreddit\n",
       "0  NHS health care..told I don’t need further help..  depression\n",
       "1                              I just want to end it  depression\n",
       "2  Sort of a random question but how bad is it if...  depression\n",
       "3                                     I feel nothing  depression\n",
       "4                                  im fucking stupid  depression"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sending Depression to depression.csv.\n",
      "\n",
      "\n",
      "Getting posts from Anxiety.\n",
      "We are on round 1, total of 0 posts.\n",
      "We are on round 2, total of 25 posts.\n",
      "We are on round 3, total of 50 posts.\n",
      "We are on round 4, total of 75 posts.\n",
      "We are on round 5, total of 100 posts.\n",
      "We are on round 6, total of 125 posts.\n",
      "We are on round 7, total of 150 posts.\n",
      "We are on round 8, total of 175 posts.\n",
      "We are on round 9, total of 200 posts.\n",
      "We are on round 10, total of 225 posts.\n",
      "We are on round 11, total of 250 posts.\n",
      "We are on round 12, total of 275 posts.\n",
      "We are on round 13, total of 300 posts.\n",
      "We are on round 14, total of 325 posts.\n",
      "We are on round 15, total of 350 posts.\n",
      "We are on round 16, total of 375 posts.\n",
      "We are on round 17, total of 400 posts.\n",
      "We are on round 18, total of 425 posts.\n",
      "We are on round 19, total of 450 posts.\n",
      "We are on round 20, total of 475 posts.\n",
      "We are on round 21, total of 500 posts.\n",
      "We are on round 22, total of 525 posts.\n",
      "We are on round 23, total of 550 posts.\n",
      "We are on round 24, total of 575 posts.\n",
      "We are on round 25, total of 600 posts.\n",
      "We are on round 26, total of 625 posts.\n",
      "We are on round 27, total of 650 posts.\n",
      "We are on round 28, total of 675 posts.\n",
      "We are on round 29, total of 700 posts.\n",
      "We are on round 30, total of 725 posts.\n",
      "We are on round 31, total of 750 posts.\n",
      "We are on round 32, total of 775 posts.\n",
      "We are on round 33, total of 800 posts.\n",
      "We are on round 34, total of 825 posts.\n",
      "We are on round 35, total of 850 posts.\n",
      "We are on round 36, total of 875 posts.\n",
      "We are on round 37, total of 900 posts.\n",
      "We are on round 38, total of 925 posts.\n",
      "We are on round 39, total of 950 posts.\n",
      "We are on round 40, total of 975 posts.\n",
      "Finished. 1000 total posts.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>can anxiety create the symptoms of what seems ...</td>\n",
       "      <td>anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anxiety makes me want to move back home</td>\n",
       "      <td>anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phone anxiety</td>\n",
       "      <td>anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i have therapy today for the first time in six...</td>\n",
       "      <td>anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Fear of Failure is Holding Me Back, and I ...</td>\n",
       "      <td>anxiety</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title subreddit\n",
       "0  can anxiety create the symptoms of what seems ...   anxiety\n",
       "1            Anxiety makes me want to move back home   anxiety\n",
       "2                                      Phone anxiety   anxiety\n",
       "3  i have therapy today for the first time in six...   anxiety\n",
       "4  The Fear of Failure is Holding Me Back, and I ...   anxiety"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sending Anxiety to anxiety.csv.\n",
      "\n",
      "\n",
      "Sending combined dataframe, full_df, to depressionanxiety.csv.\n",
      "\n",
      "\n",
      "Null Values:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "title        0\n",
       "subreddit    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "## Baseline\n",
      "Depression = 1, Anxiety = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.500501\n",
       "1    0.499499\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Doing train test split...\n",
      "Done.\n",
      "\n",
      "\n",
      "All operations completed. \n",
      " Ready for modeling.\n"
     ]
    }
   ],
   "source": [
    "test.get_subreddits(1000, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling with estimator: CountVectorizer\n",
      "Modeling with model: Logistic Regression\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed Fitting: 0:00:01.072916\n",
      "\n",
      "\n",
      "Modeling with estimator: TfidVectorizer\n",
      "Modeling with model: Logistic Regression\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed Fitting: 0:00:00.558386\n",
      "\n",
      "\n",
      "Modeling with estimator: CountVectorizer\n",
      "Modeling with model: Random Forest\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed Fitting: 0:00:00.750078\n",
      "\n",
      "\n",
      "Modeling with estimator: TfidVectorizer\n",
      "Modeling with model: Random Forest\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed Fitting: 0:00:00.837163\n",
      "\n",
      "\n",
      "Modeling with estimator: CountVectorizer\n",
      "Modeling with model: K Nearest Neighbors\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed Fitting: 0:00:00.545957\n",
      "\n",
      "\n",
      "Modeling with estimator: TfidVectorizer\n",
      "Modeling with model: K Nearest Neighbors\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Time Elapsed Fitting: 0:00:00.036035\n",
      "\n",
      "\n",
      "Modeling with estimator: CountVectorizer\n",
      "Modeling with model: Multinomial Bayes Classifier\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Time Elapsed Fitting: 0:00:00.031394\n",
      "\n",
      "\n",
      "Modeling with estimator: TfidVectorizer\n",
      "Modeling with model: Multinomial Bayes Classifier\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Time Elapsed Fitting: 0:00:00.024037\n",
      "\n",
      "\n",
      "Modeling with estimator: CountVectorizer\n",
      "Modeling with model: Support Vector Classifier\n",
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed Fitting: 0:00:01.188063\n",
      "\n",
      "\n",
      "Modeling with estimator: TfidVectorizer\n",
      "Modeling with model: Support Vector Classifier\n",
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed Fitting: 0:00:00.980558\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preprocessor</th>\n",
       "      <th>Estimator</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>Best Train Score</th>\n",
       "      <th>Best Test Score</th>\n",
       "      <th>Variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'count_vec__max_df': 0.3, 'count_vec__max_fea...</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidVectorizer</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'lr__C': 1, 'lr__penalty': 'l1', 'tfidf__max_...</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.126667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'count_vec__max_df': 0.3, 'count_vec__max_fea...</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TfidVectorizer</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'rf__max_depth': 200, 'rf__min_samples_leaf':...</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-0.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>K Nearest Neighbors</td>\n",
       "      <td>{'count_vec__max_df': 0.3, 'count_vec__max_fea...</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.306667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TfidVectorizer</td>\n",
       "      <td>K Nearest Neighbors</td>\n",
       "      <td>{'knn__metric': 'manhattan', 'knn__n_neighbors...</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>Multinomial Bayes Classifier</td>\n",
       "      <td>{'count_vec__max_df': 0.3, 'count_vec__max_fea...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TfidVectorizer</td>\n",
       "      <td>Multinomial Bayes Classifier</td>\n",
       "      <td>{'mnb__alpha': 0, 'mnb__fit_prior': False, 'tf...</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.84</td>\n",
       "      <td>-0.153333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>Support Vector Classifier</td>\n",
       "      <td>{'count_vec__max_df': 0.3, 'count_vec__max_fea...</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TfidVectorizer</td>\n",
       "      <td>Support Vector Classifier</td>\n",
       "      <td>{'svc__C': 1, 'svc__degree': 1, 'svc__gamma': ...</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Preprocessor                     Estimator  \\\n",
       "0  CountVectorizer           Logistic Regression   \n",
       "1   TfidVectorizer           Logistic Regression   \n",
       "2  CountVectorizer                 Random Forest   \n",
       "3   TfidVectorizer                 Random Forest   \n",
       "4  CountVectorizer           K Nearest Neighbors   \n",
       "5   TfidVectorizer           K Nearest Neighbors   \n",
       "6  CountVectorizer  Multinomial Bayes Classifier   \n",
       "7   TfidVectorizer  Multinomial Bayes Classifier   \n",
       "8  CountVectorizer     Support Vector Classifier   \n",
       "9   TfidVectorizer     Support Vector Classifier   \n",
       "\n",
       "                                         Best Params  Best Train Score  \\\n",
       "0  {'count_vec__max_df': 0.3, 'count_vec__max_fea...          0.706667   \n",
       "1  {'lr__C': 1, 'lr__penalty': 'l1', 'tfidf__max_...          0.566667   \n",
       "2  {'count_vec__max_df': 0.3, 'count_vec__max_fea...          0.760000   \n",
       "3  {'rf__max_depth': 200, 'rf__min_samples_leaf':...          0.993333   \n",
       "4  {'count_vec__max_df': 0.3, 'count_vec__max_fea...          0.706667   \n",
       "5  {'knn__metric': 'manhattan', 'knn__n_neighbors...          0.646667   \n",
       "6  {'count_vec__max_df': 0.3, 'count_vec__max_fea...          0.700000   \n",
       "7  {'mnb__alpha': 0, 'mnb__fit_prior': False, 'tf...          0.993333   \n",
       "8  {'count_vec__max_df': 0.3, 'count_vec__max_fea...          0.720000   \n",
       "9  {'svc__C': 1, 'svc__degree': 1, 'svc__gamma': ...          0.986667   \n",
       "\n",
       "   Best Test Score  Variance  \n",
       "0             0.64 -0.066667  \n",
       "1             0.44 -0.126667  \n",
       "2             0.44 -0.320000  \n",
       "3             0.76 -0.233333  \n",
       "4             0.40 -0.306667  \n",
       "5             0.58 -0.066667  \n",
       "6             0.64 -0.060000  \n",
       "7             0.84 -0.153333  \n",
       "8             0.68 -0.040000  \n",
       "9             0.82 -0.166667  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test.all_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Maxtrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions Across Classes (Outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORD CLOUDS!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
